{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, permutation_test_score\n",
    "\n",
    "from sklearn import gaussian_process\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "SEED = 2137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.load_scenarios import load_scenario_df\n",
    "from util.load_evaluations import load_benchmark_df\n",
    "\n",
    "eval_df = load_benchmark_df(\"../data/evaluation/benchmarking/default\")\n",
    "scenario_df = load_scenario_df(\"../data/definition/routes_devtest_sliced.xml\")\n",
    "\n",
    "# join scenario and evaluation data\n",
    "df = eval_df.join(scenario_df, on='route_index', how='inner').sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 128 entries, 0 to 131\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   duration_game        128 non-null    float64\n",
      " 1   duration_system      128 non-null    float64\n",
      " 2   route_length         128 non-null    float64\n",
      " 3   score_composed       128 non-null    float64\n",
      " 4   score_penalty        128 non-null    float64\n",
      " 5   score_route          128 non-null    float64\n",
      " 6   driving_score        128 non-null    float64\n",
      " 7   driving_score_error  128 non-null    float64\n",
      " 8   n_points             128 non-null    float64\n",
      " 9   length               128 non-null    float64\n",
      " 10  dist                 128 non-null    float64\n",
      " 11  dist_len_ratio       128 non-null    float64\n",
      " 12  max_angles           128 non-null    float64\n",
      " 13  avg_angles           128 non-null    float64\n",
      " 14  n_turns              128 non-null    float64\n",
      "dtypes: float64(15)\n",
      "memory usage: 16.0 KB\n"
     ]
    }
   ],
   "source": [
    "# use only numerical columns\n",
    "df = df.select_dtypes(include=np.number)\n",
    "\n",
    "# use 10fps only\n",
    "df = df.loc[10, \"True\"]\n",
    "\n",
    "# aggregate repetitions\n",
    "df = df.groupby('route_index').mean()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "safe_threshold = 0.01\n",
    "risky_threshold = 0.25\n",
    "\n",
    "df.loc[:, 'label'] = np.select(\n",
    "    [df['driving_score_error'] < safe_threshold,\n",
    "     df['driving_score_error'].between(safe_threshold, risky_threshold),\n",
    "     df['driving_score_error'] > risky_threshold,\n",
    "     ],\n",
    "    ['safe', 'moderate', 'risky',],\n",
    ")\n",
    "# shuffle data rows\n",
    "df = df.sample(frac=1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get featurers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['max_angles', 'dist_len_ratio', 'n_turns']\n",
    "X = df[features].to_numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['label'].to_numpy()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(kernel='linear'): \n",
      " - accuracy: 0.70 +/- 0.07\n",
      "SVC(): \n",
      " - accuracy: 0.68 +/- 0.06\n",
      "DecisionTreeClassifier(): \n",
      " - accuracy: 0.49 +/- 0.05\n",
      "GaussianProcessClassifier(): \n",
      " - accuracy: 0.59 +/- 0.07\n",
      "RandomForestClassifier(): \n",
      " - accuracy: 0.59 +/- 0.05\n",
      "ExtraTreesClassifier(): \n",
      " - accuracy: 0.59 +/- 0.04\n",
      "KNeighborsClassifier(): \n",
      " - accuracy: 0.63 +/- 0.06\n"
     ]
    }
   ],
   "source": [
    "methods = [\n",
    "\n",
    "    svm.SVC(kernel='linear'),\n",
    "    svm.SVC(kernel='rbf'),\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "]\n",
    "\n",
    "for clf in methods:\n",
    "    scores = cross_val_score(clf, X, y)\n",
    "    print(f\"{clf}: \\n - accuracy: {scores.mean():.2f} +/- {scores.std():.2f}\")\n",
    "\n",
    "    # score, permutation_scores, pvalue = permutation_test_score(\n",
    "    #     clf, X, y, random_state=0, n_jobs=16)\n",
    "\n",
    "    # print(\n",
    "    #     f\"- permutation scores: {permutation_scores.mean():.2f} +/- \"\n",
    "    #     f\"{permutation_scores.std():.2f} \\n\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple model achieve 'high' accuracy\n",
    "- P scores are slighly worring (idk if they should)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
